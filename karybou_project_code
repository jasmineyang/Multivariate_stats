library(dplyr)
library(fastDummies)
library(anytime)
library(ggplot2)
library(hrbrthemes)
library(plyr)
library(zoo)
library(cowplot)
library(recommenderlab)
library(reshape2)
library(sparsesvd)
library(ggpubr)
library(rstan)
library(coda)
library(Rlab)

# Data pre-processing
df_questions = read.csv("/Users/ue2121/Desktop/Amine/data/questions.csv")
df_questions$timestamp = as.POSIXct(df_questions$event_timestamp/1000000, origin="1970-01-01")
df_questions$correct_string <- as.integer(as.logical(df_questions$correct_string))
df_questions1 = mutate(df_questions, correct_sum = coalesce(correct_double,correct_string))

# Descriptives and graphs
summary(df_questions1)
sd(df_questions1$my_level)
sd(df_questions1$opp_level)
sd(df_questions1$elapsed_time)
sd(df_questions1$correct_sum)

X2 <- as.factor(df_questions1$question_topic) 

e_time <- ggplot(df_questions1, aes(x = elapsed_time)) +
          geom_histogram(aes(fill = ..count..), binwidth = 0.7)+
          scale_x_continuous(name = "Elapsed time in seconds") +
          scale_y_continuous(name = "Count")

df_questions1<-ddply(df_questions1,.(elapsed_time),summarise,
              prop=prop.table(table(correct_sum)),
              correct_sum1=names(table(correct_sum)))

p<-ggplot(df_questions1,aes(correct_sum1,prop,fill=elapsed_time))+
   geom_bar(stat="identity", width = 0.6, position='dodge')+
   labs(x="Response to the question",y="Proportion of time spent")

topics<-ggplot(df_questions1, aes(x=X2)) +
        geom_bar(fill="dodgerblue3",aes(y=..count../sum(..count..)))+
        labs(x="",y="Proportion")
        
# Data pre-processing to create user-item matrix
df_agg = df_questions1 %>%
  select(user_id, arena_id,event_timestamp, timestamp, correct_sum,question_id) %>%
  group_by(user_id) %>%
  arrange(event_timestamp,.by_group = TRUE) %>%
  mutate(delta_time = difftime(timestamp,lag(timestamp),units = "mins")) %>%
  mutate(new_session = ifelse(delta_time > 30,0, 1))

i = 1
j = 1
session_id = c()
for (i in 1:nrow(df_agg)){
  if (df_agg[i,8] == 0 || is.na(df_agg[i,8])){
    session_id = c(session_id,j+1)
    j = j + 1
  } else{
    session_id = c(session_id,j)
  }
}
session_id = session_id-1
df_cleaned = cbind(df_agg,session_id = session_id[1:13988])

# Implement Metric 1

df_cleaned$rating<-NA

i = 1
for (i in 1:nrow(df_cleaned)){
  if (!is.na(df_cleaned[i,8]) & df_cleaned[i,8] == 0){
    if (df_cleaned[i,7] > 0 & df_cleaned[i,7] <= 3 ){
  df_cleaned$rating[i] <- 3
  } else if (df_cleaned[i,7] > 3 & df_cleaned[i,7] <= 24){
  df_cleaned$rating[i] <- 2}
  else if (df_cleaned[i,7] > 24){
    df_cleaned$rating[i] <- 1} 
  i = i+1
}}

df = df_cleaned %>%
  select(user_id,session_id,new_session,delta_time,timestamp,question_id) %>%
  group_by(session_id) %>%
  mutate(rating_fill = ifelse(is.na(new_session) == TRUE,NA,na.locf(df_cleaned$rating,fromLast = TRUE)))
  
# Implement Metric 1

# Create week_id
df_cleaned= df_cleaned[order(df_cleaned$timestamp),] %>%
  mutate(byweek = cut(timestamp, breaks='183 hours')) %>%  # note: 62 days 
  transform(week_id = as.numeric(factor(byweek))) #8 weeks

df_cleaned= df_cleaned[order(df_cleaned$timestamp),] %>%
  mutate(byweek = cut(timestamp, breaks='183 hours')) %>%  # note: 62 days 
  transform(week_id = as.numeric(factor(byweek))) #8 weeks


# Collapse the count 
df_count = df_cleaned %>%
            group_by(user_id,week_id) %>%
            summarize(count = length(user_id))

summary(df_count)

df_count$ratings = NA

medians = list()

for (i in c(1:8)){
    medians = c(medians, median(df_count[df_count$week_id == i,]$count))
}

medians = unlist(medians)
medians[1]

for (j in c(1:nrow(df_count))){
  if (df_count[j,3] >= medians[unlist(df_count[j,2])]) {
    df_count[j,4] = 1
  } else{
    df_count[j,4] = 0
  }
}

# Plot the histograms for the number of games played each week

h1 <- ggplot(as.data.frame(df_count$count[df_count$week_id == 1]), aes(x = df_count$count[df_count$week_id == 1])) +
      geom_histogram(aes(fill = ..count..), binwidth = 7)+
      geom_vline(xintercept=median(df_count$count[df_count$week_id == 1]), size = 1, colour = "#FF3721")+
      labs(x="", y="Frequency",title="Number of games played in week 1")+
      theme(plot.title = element_text(size=15))
      
# Construct the user-item matrix using metric 2

df_2 = merge(df_cleaned,df_count,by = c("user_id","week_id"))
df_recommend = df_2[,c("user_id","question_id","ratings")]

r <- as(as.data.frame(df_recommend), "binaryRatingMatrix")
g = getRatingMatrix(r)
g
n_recommendations <- c(1,5,seq(10,100,10))


test_index <- evaluationScheme(r, method="cross", train=0.9, given=1, goodRating = 0.5)

# Show predictions for UBCF
rec <- Recommender(getData(test_index, "train"), method = "UBCF")
recom <- predict(rec, getData(test_index, "known"), n = 7,type="ratings")
result <- evaluate(x = test_index, method = "UBCF", n = n_recommendations)

r.ubcf<- Recommender(r[1:124], method = "UBCF")
recom <- predict(r.ubcf, r[125:129], n =7)
as(recom, "list")


# Matrix factorization 
r <- as(as.data.frame(df_recommend), "realRatingMatrix")
g = getRatingMatrix(r)
g
hist(getRatings(r), breaks="FD")
df=as(r, "data.frame")
summary(df)
df[df$rating==-3|df$rating==-2|df$rating==3|df$rating==2|df$rating==4|df$rating==5,"rating"] <- NA
summary(df) 

smp_size <- floor(0.9 * nrow(df)) # 90% train 
train_ind <- sample(1: nrow(df), size = smp_size)
train <- df[train_ind, ]  
test <- df[-train_ind, ]  

train_data <- data_memory(user_index = train$user, item_index = train$item, 
                          rating = train$rating, index1 = T)
test_data <- data_memory(user_index = test$user, item_index = test$item, 
                         rating = test$rating, index1 = T)
recommender <- Reco()

# set parameters arbitrarily
# dim is number of factors, (costp_12,costq_12) is regularization for user and question factors 
# lrate is learning rate, niter is number of iteractions which control convergence 
# nthread controls parallelization 
recommender$train(train_data, opts = c(dim = 10, costp_l2 = 0.1, costq_l2 = 0.1, 
                                       lrate = 0.1, niter = 100, nthread = 6, verbose = F)) 
test$prediction <- recommender$predict(test_data, out_memory())
recom <- predict(recommender, getData(test_data), n = 7,type="ratings")

# Evaluate all the models and plot the ROC curve

models_to_evaluate <- list(popular = list(name = "POPULAR", param = NULL), UBCF_cos = list(name = "UBCF", param = list(method = "cosine")),UBCF_cor = list(name = "UBCF", param = list(method = "pearson")), random = list(name = "RANDOM", param = NULL)) 
list_results <- evaluate(x = test_index, method = models_to_evaluate, n = n_recommendations)

roc<-plot(list_results, annotate = T, legend = "topleft")


# Item response model 

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

ratings = read.csv('./myData.csv', header = TRUE, sep = ',')
attach(ratings)
#rating = rating[rating$rating != "NA",]
set.seed(908)

trainset = ratings %>%
  group_by(userId) %>%
  sample_n(floor(0.8*n()))

testset = merge(trainset,ratings,by = c("userId","questionId"),all = TRUE)
testset = testset[is.na(testset$rating.x),]

train.userId = as.numeric(factor(trainset$userId))

IRM = list(J = length(unique(userId)),
           K= max(questionId),
           N=length(trainset$rating),
           jj = train.userId,
           kk = trainset$questionId,
           y = trainset$rating)


fit_ratings = stan(file = 'Item_Response_Theory_Models.stan', data = IRM,
                   chains = 2,init = 1,warmup = 1500 )

fit_ratings_2 = stan(file = 'Item_Response_Theory_Models.stan', data = IRM,iter = 3000,
                     warmup = 1500)


fit_ratings_out = extract(fit_ratings_2 , permuted = TRUE)

trans = function(x) {
  exp(x)/(1+exp(x))
}

N = length(trainset$rating)

new_train = fit_ratings_out$log_odds
prob_train = apply(new_train,2,trans)

rowsize = nrow(prob_train)

prob = prob_train[rowsize,]
set.seed(928)
y_train = ifelse(prob >= 0.5, 1, 0)

# log loss for training
logloss_train = ifelse(trainset$rating == 1, -log(prob), -log(1-prob))
plot(prob,logloss_train)

# training error
trainerr = (length(which(y_train-trainset$rating != 0)))/N
trainerr

betas = fit_ratings_out$beta[rowsize,]
alpha = fit_ratings_out$alpha[rowsize,]
gamma = fit_ratings_out$gamma[rowsize,]
mu_beta = fit_ratings_out$mu_beta[rowsize]

#posterior prediction
test.userId = as.numeric(factor(testset$userId))

test.y = rep(0,2459)
for(i in 1:2459){
  test.y[i] = gamma[testset$questionId[i]] * (alpha[test.userId[i]] - (betas[testset$questionId[i]] + mu_beta))  
}


# logit_simple = as.matrix(simple_test) %*% betas_simple+ rnorm(n=nrow(simple_test),0,sigma)
pred_prob = trans(test.y)
yp = rbern(n = nrow(testset),prob = pred_prob)

# for (i in 1:length(pred_prob)){
#   if (is.na(pred_prob_simple[i])){
#     pred_prob_simple[i] = 1
#   }
# }
# #pred_comp = as.numeric(pred_prob > 0.5)
# 
# test error
y_test = ifelse(pred_prob >= 0.5, 1, 0)
testerr= (length(which(y_test-testset$rating.y != 0)))/N
testerr

logloss_train = ifelse(testset$rating.y == 1, -log(pred_prob), -log(1-pred_prob))
plot(pred_prob,logloss_train)

# print(fit_simple, pars=c('beta','sigma'))
# plot(fit_simple,pars=c('beta','sigma'))
# effectiveSize(fit_simple_out$beta)
# effectiveSize(fit_simple_out$sigma)

counts = table(testset$rating.y, yp)
barplot(counts, main="Car Distribution by Gears and VS",
        xlab="Number of Gears", col=c("darkblue","red"),
        legend = rownames(counts), beside=TRUE)

# create a dataset
Engagement <- c(rep("0" , 2) , rep("1" , 2))
Type <- rep(c("actual values", "predications") , 2)
Number_of_questions <-c(sum(testset$rating.y == 0), sum(yp ==0), sum(testset$rating.y ==
1),sum(yp ==1))
data <- data.frame(Engagement,Type,Number_of_questions)

# Grouped
ggplot(data, aes(x=Engagement)) +
geom_bar("dodgerblue3",aes(y=Number_of_questions))+
labs(x="",y="Proportion")

ggplot(data, aes(y=Number_of_questions , x=Engagement, fill = Type)) +
geom_histogram(stat="identity", width = 0.6, position='dodge', fill = c("blue", "dark blue"))

ggplot(data, aes(fill=condition, y=value, x=Engagement)) + 
  geom_bar(position="dodge", stat="identity")


#testing in 5 indiviudals
set.seed(908)
ps = sample(unique(userId),6)
ps


# Plot the histograms for the number of games played each week
for (i in 1:6) {
  test1 = testset[testset$userId == ps[i],]
  
  index1 = which(testset$userId == ps[i])
  testps1 = y_test[index1]
  
 # create a dataset
  specie <- c(rep("0" , 2) , rep("1" , 2))
  condition <- rep(c("actual values", "predications") , 2)
  value <- c(sum(test1$rating.y == 0), sum(testps1 ==0), sum(test1$rating.y == 1),sum(testps1 ==1))
  data <- data.frame(specie,condition,value)
  
 # Grouped
  plots[[i]] = ggplot(data, aes(fill=condition, y=value, x=specie)) + 
    geom_bar(position="dodge", stat="identity")
}

ggarrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], ncol = 3,nrow = 2)

beta = betas
1.191
par(mfrow = c(3,1))
hist(alpha, breaks = 120, col = "blue")
hist(beta, breaks = 120, col = "blue")
hist(gamma, breaks = 120, col = "blue")

#six testing recommendations
ratings$userId = factor(ratings$userId)
ratings_wide = spread(ratings, questionId, rating)
P1 = ratings_wide[ratings_wide$userId == ps[1], ]
P1_long

rec = as.data.frame(matrix(rep(0,6*5170), nrow = 6, ncol = 5170))

#have to convert matrix to dataframe to store data

for (i in 1:6) {
   rec[i,] = ratings_wide[ratings_wide$userId == ps[i],]
}
colnames(rec) = colnames(ratings_wide) #rename
rating_long <- gather(rec, questionId, rating, 2:5170, factor_key=FALSE)

rec.y = rep(0,31014)
for(i in 1:31014){
rec.y[i] = gamma[rating_long$questionId[i]] * (alpha[rating_long$userId[i]]
(betas[rating_long$questionId[i]] + mu_beta))
}

rating_long <- rating_long[ord er(rating_long$userId),]
rating_long$questionId = as.numeric(rating_long$questionId)
rec_prob = trans(rec.y)

recom = data.frame(rating_long[-3],rec_prob)
recom_order = recom %>%
    group_by(userId) %>%
    arrange(
    desc(rec_prob)
)
top_n(7)



